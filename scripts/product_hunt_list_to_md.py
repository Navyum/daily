import os
import sys
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import pytz
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tencentcloud.common import credential
from tencentcloud.tmt.v20180321 import tmt_client, models
import jieba
import jieba.analyse

producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')
tencent_secret_id = os.getenv('TENCENT_SECRET_ID')
tencent_secret_key = os.getenv('TENCENT_SECRET_KEY')

cred = credential.Credential(tencent_secret_id, tencent_secret_key)
tmt_client = tmt_client.TmtClient(cred, "ap-chengdu")

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_beijing_time(createdAt)
        self.featured = "æ˜¯" if featuredAt else "å¦"
        self.website = stripe_url_params(website)
        self.url = stripe_url_params(url)
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = "æ— å…³é”®è¯"
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)

    def fetch_og_image_url(self) -> str:
        """è·å–äº§å“çš„Open Graphå›¾ç‰‡URL"""
        response = requests.get(self.url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image:
                return og_image["content"]
            # å¤‡ç”¨:æŸ¥æ‰¾twitter:image metaæ ‡ç­¾
            twitter_image = soup.find("meta", name="twitter:image") 
            if twitter_image:
                return twitter_image["content"]
        return ""

    def translate_text(self, text: str) -> str:
        """ã€ä½¿ç”¨tencentç¿»è¯‘æ–‡æœ¬å†…å®¹ã€‘"""
        try:
            request = models.TextTranslateRequest()
            request.Source = "auto" ## en
            request.Target = "zh"
            request.SourceText = text
            request.ProjectId = 0

            # å‘é€è¯·æ±‚å¹¶è·å–ç¿»è¯‘ç»“æœ
            TextTranslateResponse = tmt_client.TextTranslate(request)
            return TextTranslateResponse.TargetText
        except Exception as e:
            print(f"Error occurred during translation: {e}")
            return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        """å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´"""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        """è¿”å›äº§å“æ•°æ®çš„Markdownæ ¼å¼"""
        """ã€ä½¿ç”¨jiebaæå–å…³é”®è¯ã€‘"""
        keywords=generate_keywords(self.translated_tagline + self.translated_description)
        
        return (
            f"## [TOP{rank}    {self.name}]({self.url})\n"
            f"![{self.name}]({self.og_image_url})<br /><br />\n"
            f"**ã€æ ‡è¯­ã€‘**ï¼š{self.translated_tagline}<br />\n"
            f"**ã€ä»‹ç»ã€‘**ï¼š{self.translated_description}<br />\n"
            f"**ã€å®˜ç½‘ã€‘**ï¼š[ç«‹å³è®¿é—®]({self.website})<br />\n"
            f"**ã€Product Huntã€‘**ï¼š[View on Product Hunt]({self.url})<br /><br />\n\n"
            f"**å…³é”®è¯**ï¼š{keywords}<br />\n"
            f"**ç¥¨æ•°**ï¼š ğŸ”º{self.votes_count}<br />\n"
            f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured}<br />\n"
            f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}<br /><br />\n\n"
            f"---\n\n"
        )

def get_producthunt_token():
    """    é€šè¿‡ client_id å’Œ client_secret è·å– Product Hunt çš„ access_token
    url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": producthunt_client_id,
        "client_secret": producthunt_client_secret,
        "grant_type": "client_credentials",
    }

    headers = {
        "Content-Type": "application/json",
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code != 200:
        raise Exception(f"Failed to obtain access token: {response.status_code}, {response.text}")

    token = response.json().get("access_token")
    """

    """ä½¿ç”¨ developer token è¿›è¡Œè®¤è¯"""
    token = os.getenv('PRODUCTHUNT_DEVELOPER_TOKEN')
    if not token:
        raise Exception("Product Hunt developer token not found in environment variables")
    return token

def fetch_product_hunt_data(date_str):
    """ä»Product Huntè·å–å‰ä¸€å¤©çš„Top 30æ•°æ®"""
    token = get_producthunt_token()
    url = "https://api.producthunt.com/v2/api/graphql"
    
    # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ä¿¡æ¯
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "DecohackBot/1.0 (https://decohack.com)",
        "Origin": "https://decohack.com",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        "Connection": "keep-alive"
    }

    # è®¾ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,  # æœ€å¤šé‡è¯•3æ¬¡
        backoff_factor=1,  # é‡è¯•é—´éš”æ—¶é—´
        status_forcelist=[429, 500, 502, 503, 504]  # éœ€è¦é‡è¯•çš„HTTPçŠ¶æ€ç 
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("https://", adapter)
    
    # GraphQL
    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < 30:
        query = base_query % (date_str, date_str, cursor)
        try:
            response = session.post(url, headers=headers, json={"query": query})
            response.raise_for_status()  # æŠ›å‡ºé200çŠ¶æ€ç çš„å¼‚å¸¸
        except requests.exceptions.RequestException as e:
            raise Exception(f"Failed to fetch data from Product Hunt: {response.status_code}, {response.text}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # åªä¿ç•™å‰30ä¸ªäº§å“
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:30]]

def generate_markdown(products, date_str):
    """ç”ŸæˆMarkdownå†…å®¹å¹¶ä¿å­˜åˆ°docsç›®å½•"""
    #markdown_content = f"# PHä»Šæ—¥çƒ­æ¦œ | {date_str}\n\n"
    markdown_content = (
        f"---\n"
        f"title: PHä»Šæ—¥çƒ­æ¦œ | {date_str}\n"
        f"date: {date_str}\n"
        f"category:\n"
        f" - PH\n"
        f"order: -1\n"
        f"---\n\n"
    )
    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    # ç¡®ä¿ docs ç›®å½•å­˜åœ¨
    os.makedirs('docs/_posts', exist_ok=True)

    # ä¿®æ”¹æ–‡ä»¶ä¿å­˜è·¯å¾„åˆ° docs ç›®å½•
    file_name = f"docs/_posts/PH-daily-{date_str}.md"
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")

def stripe_url_params(url):
    stripe_url = urljoin(url, urlparse(url).path)
    return stripe_url

def generate_keywords(content):
    tags = jieba.analyse.extract_tags(content, topK=6)
    return(",".join(tags))

def main(date_str):
    if date_str :
        print(f"date: {date_str} è„šæœ¬å‚æ•°ä¼ é€’")
    else :
        # è·å–æ˜¨å¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        date_str = yesterday.strftime('%Y-%m-%d')
        print(f"date: {date_str} è‡ªåŠ¨è·å–æ˜¨å¤©çš„æ—¥æœŸ")

    # è·å–Product Huntæ•°æ®
    products = fetch_product_hunt_data(date_str)

    # ç”ŸæˆMarkdownæ–‡ä»¶
    generate_markdown(products, date_str)
 
if __name__ == "__main__":
    date_str = None
    main(date_str)
